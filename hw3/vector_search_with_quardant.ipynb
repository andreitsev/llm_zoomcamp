{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "[Imports](#Imports)\n",
    "\n",
    "[bottom](#bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "from pprint import pprint\n",
    "import sys \n",
    "import json \n",
    "import itertools \n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date, timedelta\n",
    "from load_dotenv import load_dotenv\n",
    "load_dotenv() # OPENAI_API_KEY\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (7, 7)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to the tutorial: https://qdrant.tech/documentation/tutorials/neural-search/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run qdrant container\n",
    "\n",
    "```bash\n",
    "docker run -d -p 6333:6333 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n",
    "    --name qdrant \\\n",
    "    qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loading documents and embed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading documents\n",
    "import requests\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "docs_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonandreytsev/llm_zoomcamp_venv3.9.7/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_array: 948 x 384\n"
     ]
    }
   ],
   "source": [
    "embeddings_array = model.encode([doc['text'] for doc in documents], show_progress_bar=True)\n",
    "print(f\"embeddings_array: {embeddings_array.shape[0]:,} x {embeddings_array.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/antonandreytsev/Desktop/llm_zoomcamp/hw3\n"
     ]
    }
   ],
   "source": [
    "print(f\"cwd: {os.getcwd()}\")\n",
    "np.save(pjoin(os.getcwd(), \"startup_vectors.npy\"), embeddings_array, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initialise qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x110489d00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import client library\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "qd_client = QdrantClient(\"http://localhost:6333\")\n",
    "qd_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/27/jnsmcczn7r76jgpz12mfjvkw0000gn/T/ipykernel_96496/2342428686.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qd_client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.recreate_collection(\n",
    "    collection_name=\"ml_zoomcamp_faq\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload is now an iterator over startup data\n",
    "payload = (doc for doc in documents)\n",
    "\n",
    "# Load all vectors into memory, numpy array works as iterable for itself.\n",
    "# Other option would be to use Mmap, if you don't want to load all data into RAM\n",
    "vectors = np.load(\"./startup_vectors.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x110489d00>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.upload_collection(\n",
    "    collection_name=\"ml_zoomcamp_faq\",\n",
    "    vectors=vectors,\n",
    "    payload=payload,\n",
    "    ids=None,  # Vector ids will be assigned automatically\n",
    "    batch_size=256,  # How many vectors will be uploaded in a single request?\n",
    ")\n",
    "qd_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Search for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payloads:\n",
      "[{'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Python Kafka: ./build.sh: Permission denied Error',\n",
      "  'section': 'Module 6: streaming with kafka',\n",
      "  'text': 'Run this command in terminal in the same directory '\n",
      "          '(/docker/spark):\\n'\n",
      "          'chmod +x build.sh'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Spark docker-compose setup',\n",
      "  'section': 'Module 5: pyspark',\n",
      "  'text': 'To run spark in docker setup\\n'\n",
      "          '1. Build bitnami spark docker\\n'\n",
      "          'a. clone bitnami repo using command\\n'\n",
      "          'git clone https://github.com/bitnami/containers.git\\n'\n",
      "          '(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\n'\n",
      "          'b. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update '\n",
      "          'java and spark version as following\\n'\n",
      "          '\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n'\n",
      "          '\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\n'\n",
      "          'reference: https://github.com/bitnami/containers/issues/13409\\n'\n",
      "          'c. build docker image by navigating to above directory and running '\n",
      "          'docker build command\\n'\n",
      "          'navigate cd bitnami/spark/3.3/debian-11/\\n'\n",
      "          'build command docker build -t spark:3.3-java-17 .\\n'\n",
      "          '2. run docker compose\\n'\n",
      "          'using following file\\n'\n",
      "          '```yaml docker-compose.yml\\n'\n",
      "          \"version: '2'\\n\"\n",
      "          'services:\\n'\n",
      "          'spark:\\n'\n",
      "          'image: spark:3.3-java-17\\n'\n",
      "          'environment:\\n'\n",
      "          '- SPARK_MODE=master\\n'\n",
      "          '- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n'\n",
      "          '- SPARK_RPC_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_SSL_ENABLED=no\\n'\n",
      "          'volumes:\\n'\n",
      "          '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "          'ports:\\n'\n",
      "          \"- '8080:8080'\\n\"\n",
      "          \"- '7077:7077'\\n\"\n",
      "          'spark-worker:\\n'\n",
      "          'image: spark:3.3-java-17\\n'\n",
      "          'environment:\\n'\n",
      "          '- SPARK_MODE=worker\\n'\n",
      "          '- SPARK_MASTER_URL=spark://spark:7077\\n'\n",
      "          '- SPARK_WORKER_MEMORY=1G\\n'\n",
      "          '- SPARK_WORKER_CORES=1\\n'\n",
      "          '- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n'\n",
      "          '- SPARK_RPC_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_SSL_ENABLED=no\\n'\n",
      "          'volumes:\\n'\n",
      "          '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "          'ports:\\n'\n",
      "          \"- '8081:8081'\\n\"\n",
      "          'spark-nb:\\n'\n",
      "          'image: jupyter/pyspark-notebook:java-17.0.5\\n'\n",
      "          'environment:\\n'\n",
      "          '- SPARK_MASTER_URL=spark://spark:7077\\n'\n",
      "          'volumes:\\n'\n",
      "          '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "          'ports:\\n'\n",
      "          \"- '8888:8888'\\n\"\n",
      "          \"- '4040:4040'\\n\"\n",
      "          '```\\n'\n",
      "          'run command to deploy docker compose\\n'\n",
      "          'docker-compose up\\n'\n",
      "          'Access jupyter notebook using link logged in docker compose logs\\n'\n",
      "          'Spark master url is spark://spark:7077'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why '\n",
      "              'Spark master connection fails',\n",
      "  'section': 'Module 6: streaming with kafka',\n",
      "  'text': 'Start a new terminal\\n'\n",
      "          'Run: docker ps\\n'\n",
      "          'Copy the CONTAINER ID of the spark-master container\\n'\n",
      "          'Run: docker exec -it <spark_master_container_id> bash\\n'\n",
      "          'Run: cat logs/spark-master.out\\n'\n",
      "          'Check for the log when the error happened\\n'\n",
      "          'Google the error message from there'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'How to fix docker compose error: Error response from daemon: '\n",
      "              'pull access denied for spark-3.3.1, repository does not exist '\n",
      "              \"or may require 'docker login': denied: requested access to the \"\n",
      "              'resource is denied',\n",
      "  'section': 'Module 6: streaming with kafka',\n",
      "  'text': 'If you get this error, know that you have not built your sparks and '\n",
      "          'juypter images. This images aren’t readily available on dockerHub.\\n'\n",
      "          'In the spark folder, run ./build.sh from a bash cli to to build all '\n",
      "          'images before running docker compose'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Spark Standalone Mode on Windows',\n",
      "  'section': 'Module 5: pyspark',\n",
      "  'text': 'Open a CMD terminal in administrator mode\\n'\n",
      "          'cd %SPARK_HOME%\\n'\n",
      "          'Start a master node: bin\\\\spark-class '\n",
      "          'org.apache.spark.deploy.master.Master\\n'\n",
      "          'Start a worker node: bin\\\\spark-class '\n",
      "          'org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> '\n",
      "          '--host <IP_ADDR>\\n'\n",
      "          'bin/spark-class org.apache.spark.deploy.worker.Worker '\n",
      "          'spark://localhost:7077 --host <IP_ADDR>\\n'\n",
      "          'spark://<master_ip>:<port>: copy the address from the previous '\n",
      "          'command, in my case it was spark://localhost:7077\\n'\n",
      "          'Use --host <IP_ADDR> if you want to run the worker on a different '\n",
      "          'machine. For now leave it empty.\\n'\n",
      "          'Now you can access Spark UI through localhost:8080\\n'\n",
      "          'Homework for Module 5:\\n'\n",
      "          'Do not refer to the homework file located under /05-batch/code/. '\n",
      "          'The correct file is located under\\n'\n",
      "          'https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "text = 'How can I run spark with docker?'\n",
    "vector = model.encode(text).tolist()\n",
    "\n",
    "# Use `vector` for search for closest vectors in the collection\n",
    "collection_name = 'ml_zoomcamp_faq'\n",
    "search_result = qd_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=vector,\n",
    "    query_filter=None,  # If you don't want any filters for now\n",
    "    limit=5,  # 5 the most closest results is enough\n",
    ")\n",
    "# `search_result` contains found vector ids with similarity scores along with the stored payload\n",
    "# In this function you are interested in payload only\n",
    "payloads = [hit.payload for hit in search_result]\n",
    "print(f\"payloads:\")\n",
    "pprint(payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payloads:\n",
      "[{'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Python Kafka: ./build.sh: Permission denied Error',\n",
      "  'section': 'Module 6: streaming with kafka',\n",
      "  'text': 'Run this command in terminal in the same directory '\n",
      "          '(/docker/spark):\\n'\n",
      "          'chmod +x build.sh'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Spark docker-compose setup',\n",
      "  'section': 'Module 5: pyspark',\n",
      "  'text': 'To run spark in docker setup\\n'\n",
      "          '1. Build bitnami spark docker\\n'\n",
      "          'a. clone bitnami repo using command\\n'\n",
      "          'git clone https://github.com/bitnami/containers.git\\n'\n",
      "          '(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\n'\n",
      "          'b. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update '\n",
      "          'java and spark version as following\\n'\n",
      "          '\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n'\n",
      "          '\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\n'\n",
      "          'reference: https://github.com/bitnami/containers/issues/13409\\n'\n",
      "          'c. build docker image by navigating to above directory and running '\n",
      "          'docker build command\\n'\n",
      "          'navigate cd bitnami/spark/3.3/debian-11/\\n'\n",
      "          'build command docker build -t spark:3.3-java-17 .\\n'\n",
      "          '2. run docker compose\\n'\n",
      "          'using following file\\n'\n",
      "          '```yaml docker-compose.yml\\n'\n",
      "          \"version: '2'\\n\"\n",
      "          'services:\\n'\n",
      "          'spark:\\n'\n",
      "          'image: spark:3.3-java-17\\n'\n",
      "          'environment:\\n'\n",
      "          '- SPARK_MODE=master\\n'\n",
      "          '- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n'\n",
      "          '- SPARK_RPC_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_SSL_ENABLED=no\\n'\n",
      "          'volumes:\\n'\n",
      "          '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "          'ports:\\n'\n",
      "          \"- '8080:8080'\\n\"\n",
      "          \"- '7077:7077'\\n\"\n",
      "          'spark-worker:\\n'\n",
      "          'image: spark:3.3-java-17\\n'\n",
      "          'environment:\\n'\n",
      "          '- SPARK_MODE=worker\\n'\n",
      "          '- SPARK_MASTER_URL=spark://spark:7077\\n'\n",
      "          '- SPARK_WORKER_MEMORY=1G\\n'\n",
      "          '- SPARK_WORKER_CORES=1\\n'\n",
      "          '- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n'\n",
      "          '- SPARK_RPC_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n'\n",
      "          '- SPARK_SSL_ENABLED=no\\n'\n",
      "          'volumes:\\n'\n",
      "          '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "          'ports:\\n'\n",
      "          \"- '8081:8081'\\n\"\n",
      "          'spark-nb:\\n'\n",
      "          'image: jupyter/pyspark-notebook:java-17.0.5\\n'\n",
      "          'environment:\\n'\n",
      "          '- SPARK_MASTER_URL=spark://spark:7077\\n'\n",
      "          'volumes:\\n'\n",
      "          '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "          'ports:\\n'\n",
      "          \"- '8888:8888'\\n\"\n",
      "          \"- '4040:4040'\\n\"\n",
      "          '```\\n'\n",
      "          'run command to deploy docker compose\\n'\n",
      "          'docker-compose up\\n'\n",
      "          'Access jupyter notebook using link logged in docker compose logs\\n'\n",
      "          'Spark master url is spark://spark:7077'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why '\n",
      "              'Spark master connection fails',\n",
      "  'section': 'Module 6: streaming with kafka',\n",
      "  'text': 'Start a new terminal\\n'\n",
      "          'Run: docker ps\\n'\n",
      "          'Copy the CONTAINER ID of the spark-master container\\n'\n",
      "          'Run: docker exec -it <spark_master_container_id> bash\\n'\n",
      "          'Run: cat logs/spark-master.out\\n'\n",
      "          'Check for the log when the error happened\\n'\n",
      "          'Google the error message from there'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'How to fix docker compose error: Error response from daemon: '\n",
      "              'pull access denied for spark-3.3.1, repository does not exist '\n",
      "              \"or may require 'docker login': denied: requested access to the \"\n",
      "              'resource is denied',\n",
      "  'section': 'Module 6: streaming with kafka',\n",
      "  'text': 'If you get this error, know that you have not built your sparks and '\n",
      "          'juypter images. This images aren’t readily available on dockerHub.\\n'\n",
      "          'In the spark folder, run ./build.sh from a bash cli to to build all '\n",
      "          'images before running docker compose'},\n",
      " {'course': 'data-engineering-zoomcamp',\n",
      "  'question': 'Spark Standalone Mode on Windows',\n",
      "  'section': 'Module 5: pyspark',\n",
      "  'text': 'Open a CMD terminal in administrator mode\\n'\n",
      "          'cd %SPARK_HOME%\\n'\n",
      "          'Start a master node: bin\\\\spark-class '\n",
      "          'org.apache.spark.deploy.master.Master\\n'\n",
      "          'Start a worker node: bin\\\\spark-class '\n",
      "          'org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> '\n",
      "          '--host <IP_ADDR>\\n'\n",
      "          'bin/spark-class org.apache.spark.deploy.worker.Worker '\n",
      "          'spark://localhost:7077 --host <IP_ADDR>\\n'\n",
      "          'spark://<master_ip>:<port>: copy the address from the previous '\n",
      "          'command, in my case it was spark://localhost:7077\\n'\n",
      "          'Use --host <IP_ADDR> if you want to run the worker on a different '\n",
      "          'machine. For now leave it empty.\\n'\n",
      "          'Now you can access Spark UI through localhost:8080\\n'\n",
      "          'Homework for Module 5:\\n'\n",
      "          'Do not refer to the homework file located under /05-batch/code/. '\n",
      "          'The correct file is located under\\n'\n",
      "          'https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md'}]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import Filter\n",
    "\n",
    "# Define a filter for course\n",
    "course_filter = Filter(**{\n",
    "    \"must\": [{\n",
    "        \"key\": \"course\", # Store city information in a field of the same name \n",
    "        \"match\": { # This condition checks if payload field has the requested value\n",
    "            \"value\": 'data-engineering-zoomcamp'\n",
    "        }\n",
    "    }]\n",
    "})\n",
    "\n",
    "vector = model.encode(text).tolist()\n",
    "\n",
    "# Use `vector` for search for closest vectors in the collection\n",
    "collection_name = 'ml_zoomcamp_faq'\n",
    "search_result = qd_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=vector,\n",
    "    query_filter=course_filter,\n",
    "    limit=5,  # 5 the most closest results is enough\n",
    ")\n",
    "# `search_result` contains found vector ids with similarity scores along with the stored payload\n",
    "# In this function you are interested in payload only\n",
    "payloads = [hit.payload for hit in search_result]\n",
    "print(f\"payloads:\")\n",
    "pprint(payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bottom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
