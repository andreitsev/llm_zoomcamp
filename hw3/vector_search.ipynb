{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "[Imports](#Imports)\n",
    "\n",
    "[bottom](#bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import sys \n",
    "import json \n",
    "import itertools \n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date, timedelta\n",
    "from load_dotenv import load_dotenv\n",
    "load_dotenv() # OPENAI_API_KEY\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (7, 7)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['OPENAI_API_KEY'] = 'xxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Prepare Documents\n",
    "Import documents.json, read the file and prepare the dataset:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "\n",
    "len(documents)\n",
    "```\n",
    "\n",
    "How many records we have in the pre-processed \"documents\"?\n",
    "\n",
    "- 1000\n",
    "- 1051\n",
    "- 901\n",
    "- 948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "docs_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Create Embeddings using Pretrained Models\n",
    "\n",
    "Import sentence transformer library. Please review the Sentence Transformer pretrained documentation here: [https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)\n",
    "\n",
    "```python\n",
    "# This is a new library compared to the previous modules. \n",
    "# Please perform \"pip install sentence_transformers==2.7.0\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# if you get an error do the following:\n",
    "# 1. Uninstall numpy \n",
    "# 2. Uninstall torch\n",
    "# 3. pip install numpy==1.26.4\n",
    "# 4. pip install torch\n",
    "# run the above cell, it should work\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# if you get an error do the following:\n",
    "# 1. Uninstall numpy \n",
    "# 2. Uninstall torch\n",
    "# 3. pip install numpy==1.26.4\n",
    "# 4. pip install torch\n",
    "# run the above cell, it should work\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "questions_embeddings (type: <class 'numpy.ndarray'>) 948 x 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [00:00, 144295.26it/s]\n"
     ]
    }
   ],
   "source": [
    "questions_embeddings = model.encode([info['question'] for info in documents])\n",
    "print(f\"questions_embeddings (type: {type(questions_embeddings)}) {questions_embeddings.shape[0]:,} x {questions_embeddings.shape[1]:,}\")\n",
    "\n",
    "for i, doc in tqdm(enumerate(documents)):\n",
    "    doc['question_vector'] = questions_embeddings[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '5cca4a99c596', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'qGWRjdWnRGKNbgCTgHPxAQ', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### compared to index setting from 1st homework:\n",
    "# es_client.indices.create(\n",
    "#     index='course-questions',\n",
    "#     body={\n",
    "#         \"settings\": {\n",
    "#             \"number_of_shards\": 1,\n",
    "#             \"number_of_replicas\": 0\n",
    "#         },\n",
    "#         \"mappings\": {\n",
    "#             \"properties\": {\n",
    "#                 \"text\": {\"type\": \"text\"},\n",
    "#                 \"section\": {\"type\": \"text\"},\n",
    "#                 \"question\": {\"type\": \"text\"},\n",
    "#                 \"course\": {\"type\": \"keyword\"} \n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "index_name = \"course-questions\"\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(\n",
    "    index=index_name, \n",
    "    body={\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"section\": {\"type\": \"keyword\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"course\": {\"type\": \"keyword\"} ,\n",
    "                \"question_vector\":{\"type\":\"dense_vector\",\"dims\": 384,\"index\":True,\"similarity\": \"cosine\"\n",
    "            },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "es_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/948 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:02<00:00, 341.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# adding documents to index\n",
    "for doc in tqdm(documents):\n",
    "    try:\n",
    "        es_client.index(\n",
    "            index=index_name, \n",
    "            document=doc,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'course-questions',\n",
       "  '_id': 'UyNlsZABz2SCwBl9Q0hY',\n",
       "  '_score': 0.85204804,\n",
       "  '_source': {'question': 'Course - What are the prerequisites for this course?',\n",
       "   'course': 'data-engineering-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'dCNlsZABz2SCwBl9Q0jH',\n",
       "  '_score': 0.8381672,\n",
       "  '_source': {'question': 'How can we contribute to the course?',\n",
       "   'course': 'data-engineering-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'FiNlsZABz2SCwBl9SEq6',\n",
       "  '_score': 0.8284229,\n",
       "  '_source': {'question': 'I just joined. What should I do next? How can I access course materials?',\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus'}}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"how to enrol to course?\"\n",
    "vector_search_term = model.encode(search_term)\n",
    "\n",
    "query = {\n",
    "    \"field\": \"question_vector\",\n",
    "    \"query_vector\": vector_search_term,\n",
    "    \"k\": 3,\n",
    "    \"num_candidates\": 10_000, \n",
    "}\n",
    "res = es_client.search(\n",
    "    index=\"course-questions\", \n",
    "    knn=query,\n",
    "    source=[\"text\",\"section\",\"question\",\"course\"]\n",
    ")\n",
    "res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'course-questions',\n",
       "  '_id': 'qyNlsZABz2SCwBl9R0mc',\n",
       "  '_score': 0.82894623,\n",
       "  '_source': {'question': 'Spark docker-compose setup',\n",
       "   'course': 'data-engineering-zoomcamp',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'qiNlsZABz2SCwBl9SkpY',\n",
       "  '_score': 0.8214394,\n",
       "  '_source': {'question': 'Install docker on MacOS',\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': '5. Deploying Machine Learning Models',\n",
       "   'text': 'Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'rCNlsZABz2SCwBl9REht',\n",
       "  '_score': 0.8185122,\n",
       "  '_source': {'question': 'Docker-Compose - Which docker-compose binary to use for WSL?',\n",
       "   'course': 'data-engineering-zoomcamp',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'text': 'To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\\nuname -s  -> return Linux most likely\\nuname -m -> return \"flavor\"\\nOr try this command -\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose'}}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"What is docker and how to use docker-compose?\"\n",
    "vector_search_term = model.encode(search_term)\n",
    "\n",
    "query = {\n",
    "    \"field\": \"question_vector\",\n",
    "    \"query_vector\": vector_search_term,\n",
    "    \"k\": 3,\n",
    "    \"num_candidates\": 10_000, \n",
    "}\n",
    "res = es_client.search(\n",
    "    index=\"course-questions\", \n",
    "    knn=query,\n",
    "    source=[\"text\",\"section\",\"question\",\"course\"]\n",
    ")\n",
    "\n",
    "[\n",
    "    {k:v for k, v in doc_meta.items() if k != 'question_vector'} for doc_meta in res[\"hits\"][\"hits\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1:\n",
      "\t_index: course-questions\n",
      "\t_id: UiNlsZABz2SCwBl9Q0gu\n",
      "\t_score: 2.6432812\n",
      "\t_source:\n",
      "\t\ttext: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\t\tsection: General course-related questions\n",
      "\t\tquestion: Course - When will the course start?\n",
      "\t\tcourse: data-engineering-zoomcamp\n",
      "doc 2:\n",
      "\t_index: course-questions\n",
      "\t_id: UyNlsZABz2SCwBl9Q0hY\n",
      "\t_score: 2.6432812\n",
      "\t_source:\n",
      "\t\ttext: GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites\n",
      "\t\tsection: General course-related questions\n",
      "\t\tquestion: Course - What are the prerequisites for this course?\n",
      "\t\tcourse: data-engineering-zoomcamp\n",
      "doc 3:\n",
      "\t_index: course-questions\n",
      "\t_id: VCNlsZABz2SCwBl9Q0hj\n",
      "\t_score: 2.6432812\n",
      "\t_source:\n",
      "\t\ttext: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\t\tsection: General course-related questions\n",
      "\t\tquestion: Course - Can I still join the course after the start date?\n",
      "\t\tcourse: data-engineering-zoomcamp\n",
      "doc 4:\n",
      "\t_index: course-questions\n",
      "\t_id: VSNlsZABz2SCwBl9Q0ho\n",
      "\t_score: 2.6432812\n",
      "\t_source:\n",
      "\t\ttext: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "\t\tsection: General course-related questions\n",
      "\t\tquestion: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "\t\tcourse: data-engineering-zoomcamp\n",
      "doc 5:\n",
      "\t_index: course-questions\n",
      "\t_id: ViNlsZABz2SCwBl9Q0ht\n",
      "\t_score: 2.6432812\n",
      "\t_source:\n",
      "\t\ttext: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaconda)\n",
      "Terraform\n",
      "Git\n",
      "Look over the prerequisites and syllabus to see if you are comfortable with these subjects.\n",
      "\t\tsection: General course-related questions\n",
      "\t\tquestion: Course - What can I do before the course starts?\n",
      "\t\tcourse: data-engineering-zoomcamp\n"
     ]
    }
   ],
   "source": [
    "knn_query = {\n",
    "    \"field\": \"question_vector\",\n",
    "    \"query_vector\":  vector_search_term,\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 10000\n",
    "}\n",
    "\n",
    "response = es_client.search(\n",
    "    index=index_name,\n",
    "    query={\n",
    "        \"match\": {\n",
    "                \"section\": \"General course-related questions\"\n",
    "            },\n",
    "        },\n",
    "    knn=knn_query,\n",
    "    size=5\n",
    ")\n",
    "\n",
    "for i, doc_meta in enumerate(response[\"hits\"][\"hits\"], start=1):\n",
    "    print(f\"doc {i}:\")\n",
    "    for k, v in doc_meta.items():\n",
    "        if k != '_source':\n",
    "            print(f\"\\t{k}: {v}\")\n",
    "        else:\n",
    "            print(f\"\\t{k}:\")\n",
    "            for k_, v_ in v.items():\n",
    "                if k_ != 'question_vector':\n",
    "                    print(f\"\\t\\t{k_}: {v_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Getting the embeddings model\n",
    "First, we will get the embeddings model `multi-qa-distilbert-cos-v1` from the Sentence Transformer library\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "```\n",
    "\n",
    "Create the embedding for this user question:\n",
    "\n",
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "What's the first value of the resulting vector?\n",
    "\n",
    "- -0.24\n",
    "- -0.04\n",
    "- 0.07\n",
    "- 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonandreytsev/llm_zoomcamp_venv3.9.7/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = 'multi-qa-distilbert-cos-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07822261"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "embedding_model.encode(user_question)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the documents\n",
    "Now we will create the embeddings for the documents.\n",
    "\n",
    "Load the documents with ids that we prepared in the module:\n",
    "```python\n",
    "import requests \n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()\n",
    "```\n",
    "\n",
    "We will use only a subset of the questions - the questions for \"machine-learning-zoomcamp\". After filtering, you should have only 375 documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Creating the embeddings\n",
    "Now for each document, we will create an embedding for both question and answer fields.\n",
    "\n",
    "We want to put all of them into a single matrix X:\n",
    "\n",
    "- Create a list embeddings\n",
    "- Iterate over each document\n",
    "- `qa_text = f'{question} {text}'`\n",
    "- compute the embedding for qa_text, append to embeddings\n",
    "- At the end, let `X = np.array(embeddings)` (`import numpy as np`)\n",
    "- What's the shape of X? (`X.shape`). Include the parantheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(948, 768)\n"
     ]
    }
   ],
   "source": [
    "embeds_array = embedding_model.encode([\n",
    "    f\"{doc_meta['question']} {doc_meta['text']}\"\n",
    "    for doc_meta in documents\n",
    "])\n",
    "print(embeds_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Search\n",
    "\n",
    "We have the embeddings and the query vector. Now let's compute the cosine similarity between the vector from Q1 (let's call it v) and the matrix from Q2.\n",
    "\n",
    "The vectors returned from the embedding model are already normalized (you can check it by computing a dot product of a vector with itself - it should return something very close to 1.0). This means that in order to compute the coside similarity, it's sufficient to multiply the matrix X by the vector v:\n",
    "\n",
    "`scores = X.dot(v)`\n",
    "What's the highest score in the results?\n",
    "\n",
    "- 65.0\n",
    "- 6.5\n",
    "- 0.65\n",
    "- 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6506573641439212"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "(\n",
    "    1 - cdist(\n",
    "        XA=embedding_model.encode(user_question).reshape(1, -1),\n",
    "        XB=embeds_array,\n",
    "        metric='cosine'\n",
    "    ).ravel()\n",
    ").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65065736"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embeds_array @ embedding_model.encode(user_question)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector search\n",
    "\n",
    "We can now compute the similarity between a query vector and all the embeddings.\n",
    "\n",
    "Let's use this to implement our own vector search\n",
    "\n",
    "```python\n",
    "class VectorSearchEngine():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query, num_results=10):\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        idx = np.argsort(-scores)[:num_results]\n",
    "        return [self.documents[i] for i in idx]\n",
    "\n",
    "search_engine = VectorSearchEngine(documents=documents, embeddings=X)\n",
    "search_engine.search(v, num_results=5)\n",
    "```\n",
    "\n",
    "If you don't understand how the search function work:\n",
    "\n",
    "Ask ChatGTP or any other LLM of your choice to explain the code\n",
    "Check our pre-course workshop about implementing a search engine here\n",
    "(Note: you can replace argsort with argpartition to make it a lot faster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': 'c02e79ef'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user question:\n",
      "I just discovered the course. Can I still join it?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'The course has already started. Can I still join it?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ee58a693'},\n",
       " {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7842b56a'},\n",
       " {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2f19301f'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a482086d'},\n",
       " {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c02e79ef'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing as t \n",
    "import numpy.typing as npt \n",
    "\n",
    "DocumentType = t.Dict[str, str]\n",
    "\n",
    "class VectorSearchEngine():\n",
    "    def __init__(self, documents: t.List[DocumentType], embeddings: npt.NDArray[float]):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query: np.ndarray, num_results=10) -> t.List[DocumentType]:\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        idx = np.argsort(-scores)[:num_results]\n",
    "        return [self.documents[i] for i in idx]\n",
    "\n",
    "print(\"user question:\")\n",
    "print(user_question, end='\\n'*2)\n",
    "query_vector = embedding_model.encode(user_question)\n",
    "search_engine = VectorSearchEngine(documents=documents, embeddings=embeds_array)\n",
    "search_engine.search(query_vector, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Hit-rate for our search engine\n",
    "\n",
    "Let's evaluate the performance of our own search engine. We will use the hitrate metric for evaluation.\n",
    "\n",
    "First, load the ground truth dataset:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "```\n",
    "\n",
    "Now use the code from the module to calculate the hitrate of VectorSearchEngine with num_results=5.\n",
    "\n",
    "What did you get?\n",
    "\n",
    "- 0.93\n",
    "- 0.73\n",
    "- 0.53\n",
    "- 0.33\n",
    "\n",
    "---\n",
    "Hit Rate (HR) or Recall at k:\n",
    "\n",
    "Measures the proportion of queries for which at least one relevant document is retrieved in the top k results.\n",
    "Formula: HR@k = (Number of queries with at least one relevant document in top k) / |Q|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [00:43<00:00, 42.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9218579234972678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# computing hit-rate\n",
    "denominator = len(ground_truth)\n",
    "numerator = 0\n",
    "num_results = 5\n",
    "for doc_meta in tqdm(ground_truth):\n",
    "    question, relevant_doc = doc_meta['question'], doc_meta['document']\n",
    "    question_vec = embedding_model.encode(question)\n",
    "    results = search_engine.search(question_vec, num_results)\n",
    "    retrieved_documents = [doc_meta_['id'] for doc_meta_ in results]\n",
    "    if relevant_doc in retrieved_documents:\n",
    "        numerator += 1\n",
    "print(numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Indexing with Elasticsearch\n",
    "\n",
    "Now let's index these documents with elasticsearch\n",
    "\n",
    "- Create the index with the same settings as in the module (but change the dimensions)\n",
    "- Index the embeddings (note: you've already computed them)\n",
    "\n",
    "After indexing, let's perform the search of the same query from Q1.\n",
    "\n",
    "What's the ID of the document with the highest score?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_array = embedding_model.encode([\n",
    "    f\"{doc_meta['question']} {doc_meta['text']}\"\n",
    "    for doc_meta in documents\n",
    "])\n",
    "print(embeds_array.shape)\n",
    "\n",
    "for i, doc_meta in tqdm(enumerate(documents)):\n",
    "    doc_meta['question_and_text_vector'] = embeds_array[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '5cca4a99c596', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'qGWRjdWnRGKNbgCTgHPxAQ', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"course-questions\"\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(\n",
    "    index=index_name, \n",
    "    body={\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"section\": {\"type\": \"keyword\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"course\": {\"type\": \"keyword\"} ,\n",
    "                #\"id\": {'type': \"text\"},\n",
    "                \"question_and_text_vector\":{\n",
    "                    \"type\":\"dense_vector\",\n",
    "                    \"dims\": embeds_array.shape[1],\n",
    "                    \"index\":True,\n",
    "                    \"similarity\": \"cosine\"\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "es_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/948 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:02<00:00, 364.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# adding documents to index\n",
    "for doc in tqdm(documents):\n",
    "    try:\n",
    "        es_client.index(\n",
    "            index=index_name, \n",
    "            document=doc,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'course-questions',\n",
       "  '_id': 'XyNesZABz2SCwBl9wkZJ',\n",
       "  '_score': 0.8253288,\n",
       "  '_source': {'question': 'The course has already started. Can I still join it?',\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "   'id': 'ee58a693'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'oCNesZABz2SCwBl9vUR_',\n",
       "  '_score': 0.7873188,\n",
       "  '_source': {'question': 'Course - Can I still join the course after the start date?',\n",
       "   'course': 'data-engineering-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "   'id': '7842b56a'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'wCNesZABz2SCwBl9vUTe',\n",
       "  '_score': 0.75837696,\n",
       "  '_source': {'question': 'How can we contribute to the course?',\n",
       "   'course': 'data-engineering-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
       "   'id': '2f19301f'}}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"I just discovered the course. Can I still join it?\"\n",
    "vector_search_term = embedding_model.encode(search_term)\n",
    "\n",
    "query = {\n",
    "    \"field\": \"question_and_text_vector\",\n",
    "    \"query_vector\": vector_search_term,\n",
    "    \"k\": 3,\n",
    "    \"num_candidates\": 10_000, \n",
    "}\n",
    "res = es_client.search(\n",
    "    index=\"course-questions\", \n",
    "    knn=query,\n",
    "    source=[\"text\",\"section\",\"question\",\"course\",'id']\n",
    ")\n",
    "res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Hit-rate for Elasticsearch\n",
    "\n",
    "The search engine we used in Q4 computed the similarity between the query and ALL the vectors in our database. Usually this is not practical, as we may have a lot of data.\n",
    "\n",
    "Elasticsearch uses approximate techniques to make it faster.\n",
    "\n",
    "Let's evaluate how worse the results are when we switch from exact search (as in Q4) to approximate search with Elastic.\n",
    "\n",
    "What's hitrate for our dataset for Elastic?\n",
    "\n",
    "- 0.93\n",
    "- 0.73\n",
    "- 0.53\n",
    "- 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def search_in_elastic(\n",
    "    search_query: str,\n",
    "    embedder: t.Callable[[str], npt.NDArray],\n",
    "    elastic_client: Elasticsearch,\n",
    "    query_params: t.Dict[t.Any, t.Any],\n",
    ") -> t.List[DocumentType]:\n",
    "    vector_search_term = embedder(search_query)\n",
    "    res = elastic_client.search(\n",
    "        index=\"course-questions\", \n",
    "        knn={**query_params, \"query_vector\": vector_search_term},\n",
    "        source=[\"text\",\"section\",\"question\",\"course\",\"id\"]\n",
    "    )\n",
    "    return res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1830 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [00:36<00:00, 50.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9218579234972678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# computing hit-rate for search with elastic\n",
    "denominator = len(ground_truth)\n",
    "numerator = 0\n",
    "num_results = 5\n",
    "for doc_meta in tqdm(ground_truth):\n",
    "    question, relevant_doc = doc_meta['question'], doc_meta['document']\n",
    "    results = search_in_elastic( \n",
    "        search_query=question,\n",
    "        embedder=lambda query: embedding_model.encode(query),\n",
    "        elastic_client=es_client,\n",
    "        query_params={\n",
    "            \"field\": \"question_and_text_vector\",\n",
    "            \"k\": num_results,\n",
    "            \"num_candidates\": 10_000, \n",
    "        },\n",
    "    )\n",
    "    retrieved_documents = [doc_meta_['_source']['id'] for doc_meta_ in results]\n",
    "    if relevant_doc in retrieved_documents:\n",
    "        numerator += 1\n",
    "print(numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bottom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
